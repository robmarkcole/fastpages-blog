<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The WorldStrat Dataset | Blog of Robin Cole</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="The WorldStrat Dataset" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Quick tour of the WorldStrat dataset" />
<meta property="og:description" content="Quick tour of the WorldStrat dataset" />
<link rel="canonical" href="https://robmarkcole.com/markdown/2022/07/21/worldstrat.html" />
<meta property="og:url" content="https://robmarkcole.com/markdown/2022/07/21/worldstrat.html" />
<meta property="og:site_name" content="Blog of Robin Cole" />
<meta property="og:image" content="https://robmarkcole.com/images/worldstrat/main.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-21T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://robmarkcole.com/images/worldstrat/main.jpg" />
<meta property="twitter:title" content="The WorldStrat Dataset" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-21T00:00:00-05:00","datePublished":"2022-07-21T00:00:00-05:00","description":"Quick tour of the WorldStrat dataset","headline":"The WorldStrat Dataset","image":"https://robmarkcole.com/images/worldstrat/main.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://robmarkcole.com/markdown/2022/07/21/worldstrat.html"},"url":"https://robmarkcole.com/markdown/2022/07/21/worldstrat.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://robmarkcole.com/feed.xml" title="Blog of Robin Cole" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-235705291-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-235705291-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blog of Robin Cole</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">The WorldStrat Dataset</h1><p class="page-description">Quick tour of the WorldStrat dataset</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-21T00:00:00-05:00" itemprop="datePublished">
        Jul 21, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#getting-started">Getting started</a></li>
<li class="toc-entry toc-h2"><a href="#extending-the-dataset">Extending the dataset</a></li>
<li class="toc-entry toc-h2"><a href="#super-resolution">Super resolution</a></li>
<li class="toc-entry toc-h2"><a href="#summary">Summary</a></li>
<li class="toc-entry toc-h2"><a href="#footnotes">Footnotes</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>A new dataset called the WorldStrat dataset was this week announced on Twitter by Julien Cornebise and colleagues. The release includes a paper on arXiv<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, code on Github<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, and the dataset itself on both Zenodo<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> and Kaggle<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.</p>

<blockquote class="twitter-tweet tw-align-center">
<p lang="en" dir="ltr">Proud to release the WorldStrat dataset with <a href="https://twitter.com/ivanorsolic?ref_src=twsrc%5Etfw">@ivanorsolic</a> <a href="https://twitter.com/alkalait?ref_src=twsrc%5Etfw">@alkalait</a> and <a href="https://twitter.com/esa?ref_src=twsrc%5Etfw">@esa</a> Î¦-lab:<br>- ~10,000 kmÂ² of high-res SPOT satellite imagery<br>- Multi-temporal matched low-res Sentinel2 images<br>- Stratified wrt global land-uses<br>- Enriched with sites under-represented in ML<br>ðŸ§µ1/9 <a href="https://t.co/kR0AnzIlkg">pic.twitter.com/kR0AnzIlkg</a></p>â€” Julien Cornebise (@JCornebise) <a href="https://twitter.com/JCornebise/status/1549356696664956928?ref_src=twsrc%5Etfw">July 19, 2022</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>This dataset is significant for a number of reasons:</p>

<ul>
  <li>Greater than 100 GB of imagery with global coverage</li>
  <li>Covers locations typically under-represented in ML datasets</li>
  <li>Colocated low and high resolution imagery enables training of super resolution models</li>
  <li>Code provided to extend the dataset</li>
  <li>Models &amp; tutorials provided</li>
  <li>Licenses covering non commercial use</li>
</ul>

<p>The dataset comprises Sentinel 2 timeseries &amp; colocated high resoluton Airbus SPOT imagery providing 10 &amp; 1.5 m/pixel respectively. An example is shown below:</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/worldstrat/fields.jpg" alt="" title="Fields"></p>

<h2 id="getting-started">
<a class="anchor" href="#getting-started" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting started</h2>
<p>I followed the instructions in the repository readme and downloaded the smaller dataset from Kaggle via the web UI. The zipped Kaggle dataset is 53GB, and even on a fast internet connection the download took 3-4 hours<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. I began by running the <a href="https://github.com/worldstrat/worldstrat/blob/main/Dataset%20Exploration.ipynb">Dataset Exploration.ipynb</a> notebook, which gives an introduction to the dataset. Interestingly the imagery locations were suggested by several organisations:</p>

<ul>
  <li>22 Ã— 22.5kmÂ² images of Amnesty provided locations or 198 Ã— 2.5kmÂ² images.</li>
  <li>39 Ã— 22.5kmÂ² images of ASMSpotter provided locations or 351 Ã— 2.5kmÂ² images.</li>
  <li>981 Ã— 2.5kmÂ² images of UNHCR provided locations.</li>
  <li>2,407 Ã— 2.5kmÂ² images of randomly sampled/stratified locations.</li>
</ul>

<p>Several maps are generated using <a href="http://python-visualization.github.io/folium/">folium</a> which show the capture location of the images. The image metadata is visualised in a pandas dataframe, and includes lat/lon, the source (e.g. Amnesty, UNHCR etc.) and a description field. Some very interesting locations are included, such as <code class="language-plaintext highlighter-rouge">Camp 25, North Korea- Gulag</code>, <code class="language-plaintext highlighter-rouge">USA - migrant camps on border</code> and <code class="language-plaintext highlighter-rouge">Brazil-Jaci_Deforestation_Cattle</code>. Given the provided metadata you could at this point start curating the dataset, say if you were interested in generating a version of the dataset focussed on a particular geography.</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/worldstrat/map.jpg" alt="" title="Example map generated in Dataset Exploration.ipynb"></p>

<h2 id="extending-the-dataset">
<a class="anchor" href="#extending-the-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Extending the dataset</h2>
<p>The notebook <a href="https://github.com/worldstrat/worldstrat/blob/main/Dataset%20Generation.ipynb">Dataset Generation.ipynb</a> shows how the dataset was created, and is the starting point if you want to extend the dataset yourself. To do this you can either supply a table of specific points to sample (as a csv or geojson) <strong>or</strong> you can uniformly sample the planet. There is an interesting dicussion on the challenges of random sampling and a mitigating strategy is provided.</p>

<p>To discover available imagery you will need a SentinelHub account, and <a href="https://www.sentinel-hub.com/trial">you can get a free 30-day trial account here</a>. Clearly it might take some searching and good luck to locate imagery from the two satellites, over your area of interest, and within a short time window. Fortunately code is provided to assist with searching &amp; filtering the imagery. Note that only Sentinel 2 imagery can be downloaded for free, and SPOT imagery must be purchased from Airbus. Fortunately the purchase can be performed through SentinelHub and looks pretty straightforward. Code is also provided for downloading your ordered imagery, so the whole process from discovery to ingestion of data is very well documented.</p>

<h2 id="super-resolution">
<a class="anchor" href="#super-resolution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Super resolution</h2>
<p>One of the stated aims of releasing this dataset was to enable the training of super resolution models. The hope is that by super resolving free Sentinel imagery, researchers without the budget for purchasing high resolutuion imagery (e.g. SPOT) can use super resolved &amp; free Sentinel imagery. Training super resolution models generally requires colocated low and high resolution imagery. One strategy often used is to downsample high resolution imagery to low resolution, and use these pairs for training. For example a 1.5m spot image could be downsampled to 10m and used to train a model that could then be applied to 10m Sentinel 2 imagery. However with this approach has disadvantages. Firstly the downsampled imagery may have different characteristics from the imagery you will use in practice, leading to unsatisfactory performance in real applications. Secondly, single imager super resolution (SISR) generally is outperformed by multi image super resolution (MISR), and creating a MISR dataset from high resoltuion imagery alone would multiply your costs significantly. Fortunately the WorldStrat dataset provides both imagery from the two satellites, and multiple frames from Sentinel 2, enabling the training of MISR super resolution models (as well as SISR). The Github code includes script for training super resolution models and an example output is shown below:</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/worldstrat/SR.jpg" alt="" title="Super resolution example"></p>

<p>The notebook <a href="https://github.com/worldstrat/worldstrat/blob/main/Training.ipynb">Training.ipynb</a> demonstrates how to run the training script. Whilst in general training is performed on a Linux machine, the authors have very helpfully included arguments if you wanted to perform training on a Windows machine. The code itself is written in python and uses pytorch and <a href="https://www.pytorchlightning.ai/">pytorch-lightning</a> for the model and training loop. The code also includes logging of training metrics to <a href="https://wandb.ai/site">Weights &amp; Biases</a>. The models themselves are relatively lightweight and it is suggested that training the MISR model will take 45 min - 1.5 hr on a single GPU instance. This should be within the limitations of Kaggle GPU instances so if you are interested in training the model yourself I would recommend beginning there. Note that a pretrained model is supplied in the Github repository and its use is shown in the <a href="https://github.com/worldstrat/worldstrat/blob/main/Inference.ipynb">Inference.ipynb</a> notebook. If you want to read more about super resolution see the links in my satellite-image-deep-learning Github repository <a href="https://github.com/robmarkcole/satellite-image-deep-learning#super-resolution">here</a>.</p>

<h2 id="summary">
<a class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h2>
<p>Overall I am very impressed by both the size and global nature of the dataset, the quality of the documentation and code, and the level of thought that has gone into its preparation. Datasets often focus on a particular part of the world (USA mostly) and are often released without clear documentation on how they were produced, making them hard to extend.</p>

<h2 id="footnotes">
<a class="anchor" href="#footnotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Paper on <a href="https://arxiv.org/abs/2207.06418">arXiv</a>Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Code <a href="https://github.com/worldstrat/worldstrat">on Github</a>Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>The full dataset is <a href="https://zenodo.org/record/6810792#.YtjNb-zMK3I">hosted on Zenodo</a>Â <a href="#fnref:3" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>A smaller version of the dataset <a href="https://www.kaggle.com/datasets/jucor1/worldstrat">is on Kaggle</a>.Â <a href="#fnref:4" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Alternatively I could have just created a notebook on Kaggle itself, and would not have needed to wait for a download.Â <a href="#fnref:5" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/markdown/2022/07/21/worldstrat.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
