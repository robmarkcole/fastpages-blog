<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Quick tour of the WorldStrat Dataset | Blog of Robin Cole</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Quick tour of the WorldStrat Dataset" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This significant new dataset raises the bar for documentation quality" />
<meta property="og:description" content="This significant new dataset raises the bar for documentation quality" />
<link rel="canonical" href="https://robmarkcole.com/markdown/2022/08/01/worldstrat.html" />
<meta property="og:url" content="https://robmarkcole.com/markdown/2022/08/01/worldstrat.html" />
<meta property="og:site_name" content="Blog of Robin Cole" />
<meta property="og:image" content="https://robmarkcole.com/images/worldstrat/main.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-01T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://robmarkcole.com/images/worldstrat/main.jpg" />
<meta property="twitter:title" content="Quick tour of the WorldStrat Dataset" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-01T00:00:00-05:00","datePublished":"2022-08-01T00:00:00-05:00","description":"This significant new dataset raises the bar for documentation quality","headline":"Quick tour of the WorldStrat Dataset","image":"https://robmarkcole.com/images/worldstrat/main.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://robmarkcole.com/markdown/2022/08/01/worldstrat.html"},"url":"https://robmarkcole.com/markdown/2022/08/01/worldstrat.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://robmarkcole.com/feed.xml" title="Blog of Robin Cole" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-235705291-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-235705291-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blog of Robin Cole</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Quick tour of the WorldStrat Dataset</h1><p class="page-description">This significant new dataset raises the bar for documentation quality</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-08-01T00:00:00-05:00" itemprop="datePublished">
        Aug 1, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#getting-started">Getting started</a></li>
<li class="toc-entry toc-h2"><a href="#extending-the-dataset">Extending the dataset</a></li>
<li class="toc-entry toc-h2"><a href="#super-resolution">Super resolution</a></li>
<li class="toc-entry toc-h2"><a href="#other-uses-for-the-dataset">Other uses for the dataset</a></li>
<li class="toc-entry toc-h2"><a href="#datasheet">Datasheet</a></li>
<li class="toc-entry toc-h2"><a href="#summary">Summary</a></li>
<li class="toc-entry toc-h2"><a href="#footnotes">Footnotes</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>A new dataset called the WorldStrat dataset was recently announced on Twitter by <a href="https://twitter.com/JCornebise">Julien Cornebise</a>, <a href="https://twitter.com/ivanorsolic">Ivan Orsolic</a>, and <a href="https://twitter.com/alkalait">Freddie Kalaitzis</a>. The dataset raises the bar in documentation quality of a dataset and the release includes a paper on arXiv<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, code on Github<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, and the dataset itself on both Zenodo<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> and Kaggle<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.</p>

<blockquote class="twitter-tweet tw-align-center">
<p lang="en" dir="ltr">Proud to release the WorldStrat dataset with <a href="https://twitter.com/ivanorsolic?ref_src=twsrc%5Etfw">@ivanorsolic</a> <a href="https://twitter.com/alkalait?ref_src=twsrc%5Etfw">@alkalait</a> and <a href="https://twitter.com/esa?ref_src=twsrc%5Etfw">@esa</a> Î¦-lab:<br>- ~10,000 kmÂ² of high-res SPOT satellite imagery<br>- Multi-temporal matched low-res Sentinel2 images<br>- Stratified wrt global land-uses<br>- Enriched with sites under-represented in ML<br>ðŸ§µ1/9 <a href="https://t.co/kR0AnzIlkg">pic.twitter.com/kR0AnzIlkg</a></p>â€” Julien Cornebise (@JCornebise) <a href="https://twitter.com/JCornebise/status/1549356696664956928?ref_src=twsrc%5Etfw">July 19, 2022</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>This dataset is significant for a number of reasons:</p>

<ul>
  <li>Greater than 100 GB of imagery with global coverage</li>
  <li>Covers locations typically under-represented in ML datasets</li>
  <li>Colocated low and high resolution imagery</li>
  <li>Code provided to extend the dataset &amp; integrate with the EO-Learn<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> Python package</li>
  <li>Models &amp; tutorials provided</li>
  <li>Demonstrates super resolving Sentinel 2 imagery</li>
  <li>The code, the Sentinel 2 images, the locations, the methodology, and the pre-trained models are all licensed for commercial use (SPOT imagery is NOT)</li>
</ul>

<p>The dataset comprises Sentinel 2 time-series &amp; colocated high resoluton Airbus SPOT imagery providing 10 &amp; 1.5 m/pixel respectively. An example is shown below:</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/worldstrat/fields.jpg" alt="" title="Fields"></p>

<h2 id="getting-started">
<a class="anchor" href="#getting-started" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting started</h2>
<p>I followed the instructions in the repository readme and downloaded the smaller dataset from Kaggle via the web UI. The zipped Kaggle dataset is 53GB, and even on a fast internet connection the download took 3-4 hours<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. I began by running the <a href="https://github.com/worldstrat/worldstrat/blob/main/Dataset%20Exploration.ipynb">Dataset Exploration.ipynb</a> notebook, which gives an introduction to the dataset. Interestingly the imagery locations were suggested by several organisations:</p>

<ul>
  <li>22 Ã— 22.5kmÂ² images of Amnesty provided locations or 198 Ã— 2.5kmÂ² images.</li>
  <li>39 Ã— 22.5kmÂ² images of ASMSpotter provided locations or 351 Ã— 2.5kmÂ² images.</li>
  <li>981 Ã— 2.5kmÂ² images of UNHCR provided locations.</li>
  <li>2,407 Ã— 2.5kmÂ² images of stratified-sampled locations to ensure coverage of all types of land-use and human density</li>
</ul>

<p>Several maps are generated using <a href="http://python-visualization.github.io/folium/">folium</a> which show the capture location of the images. The image metadata is visualised in a pandas dataframe, and includes lat/lon, the source (e.g. Amnesty, UNHCR etc.) and a description field. Some very interesting locations are included, such as <code class="language-plaintext highlighter-rouge">Camp 25, North Korea- Gulag</code>, <code class="language-plaintext highlighter-rouge">USA - migrant camps on border</code> and <code class="language-plaintext highlighter-rouge">Brazil-Jaci_Deforestation_Cattle</code>. Given the provided metadata you could at this point start curating the dataset, say if you were interested in generating a version of the dataset focussed on a particular geography.</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/worldstrat/map.jpg" alt="" title="Example map generated in Dataset Exploration.ipynb"></p>

<h2 id="extending-the-dataset">
<a class="anchor" href="#extending-the-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Extending the dataset</h2>
<p>The notebook <a href="https://github.com/worldstrat/worldstrat/blob/main/Dataset%20Generation.ipynb">Dataset Generation.ipynb</a> shows how the dataset was created, and is the starting point if you want to extend the dataset yourself. To do this you can either supply a table of specific points to sample (as a csv or geojson) <strong>or</strong> you can uniformly sample the planet. There is an interesting dicussion on the challenges of random sampling and a mitigating strategy is provided. To discover available imagery you will need a SentinelHub account, and <a href="https://www.sentinel-hub.com/trial">you can get a free 30-day trial account here</a>. Clearly it might take some searching and good luck to locate imagery from the two satellites, over your area of interest, and within a short time window. Fortunately code is provided to assist with searching &amp; filtering the imagery. Note that only Sentinel 2 imagery can be downloaded for free, and SPOT imagery must be purchased from Airbus. Fortunately the purchase can be performed through SentinelHub and looks pretty straightforward. Code is also provided for downloading your ordered imagery, so the whole process from discovery to ingestion of data is very well documented.</p>

<h2 id="super-resolution">
<a class="anchor" href="#super-resolution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Super resolution</h2>
<p>One of the stated aims of releasing this dataset was to enable the training of super resolution models. The hope is that by super resolving free Sentinel imagery, users without the budget for purchasing high resolution imagery (e.g. SPOT) can use super resolved &amp; free Sentinel imagery in their applications. Training super resolution models generally requires colocated low and high resolution imagery, and to this end the WorldStrat dataset provides both low and high resolution imagery. Multiple low resolution frames from Sentinel 2 are provided for each location, enabling the training of multi-image super resolution models, as well as the more typical single-image super resolution models. The Github code includes a script for training super resolution models and an example output is shown below:</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/worldstrat/SR.jpg" alt="" title="Super resolution example"></p>

<p>The notebook <a href="https://github.com/worldstrat/worldstrat/blob/main/Training.ipynb">Training.ipynb</a> demonstrates how to run the training script. Whilst in general training is performed on a Linux machine, the authors have very helpfully included arguments if you wanted to perform training on a Windows machine. The code itself is written in python and uses pytorch and <a href="https://www.pytorchlightning.ai/">pytorch-lightning</a> for the model and training loop. The code also includes logging of training metrics to <a href="https://wandb.ai/site">Weights &amp; Biases</a>. The models themselves are relatively lightweight and it is suggested that training the multi-image super resolution model will take 45 min - 1.5 hr on a single GPU instance. This should be within the limitations of a Kaggle GPU instance, so if you are interested in training the model yourself I would recommend beginning there. Note that a pretrained model is supplied in the Github repository and its use is shown in the <a href="https://github.com/worldstrat/worldstrat/blob/main/Inference.ipynb">Inference.ipynb</a> notebook. If you want to read more about super resolution read section 4.1 of the ArXiv paper and see the links in my satellite-image-deep-learning Github repository <a href="https://github.com/robmarkcole/satellite-image-deep-learning#super-resolution">here</a>.</p>

<h2 id="other-uses-for-the-dataset">
<a class="anchor" href="#other-uses-for-the-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other uses for the dataset</h2>
<p>The availability of high resolution imagery is of significant interest in itself, since this is typically very expensive to acquire. This dataset covers locations typically under-represented in high resolution datasets and so will be of significant interest to those who study those locations. All images in this dataset are provided without annotation, so their use in applications such as segmentation or object detection would first require annotation to be performed<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. In the ArXiv paper they note: â€˜Because every image is geo-referenced and timestamped, it is also possible to cross-reference it with any other source of label, for example mapping databases like OpenStreetMap, for building imprints, structure detection, etcâ€™. However there are a number of unsupervised or self-supervised techniques which do not required annotated data. In particular self supervised change detection is a technique that can be performed on the Sentinel 2 time series imagery, and where having the high resolution reference could aid with evaluation of predictions. A good introductory read on this topic is the paper <a href="https://arxiv.org/abs/2105.08501v2">Self-supervised Remote Sensing Images Change Detection at Pixel-level</a></p>

<h2 id="datasheet">
<a class="anchor" href="#datasheet" aria-hidden="true"><span class="octicon octicon-link"></span></a>Datasheet</h2>
<p>The appendix of the ArXiv paper includes a <code class="language-plaintext highlighter-rouge">Datasheet</code> for the dataset which describes the motivation, composition, collection process &amp; recommended uses for the dataset following a methodology presented in the paper Datasheets for Datasets<sup id="fnref:7:1" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. The purpose of including this datasheet is to improve &amp; standardise the quality of the documentation, in order to enable the use of the dataset with confidence and provide transparency and accountability around the production of the dataset itself. I think this is a very welcome inclusion and I hope to see more datasets published with an accompanying Datasheet.</p>

<h2 id="summary">
<a class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h2>
<p>Overall I am very impressed by both the size and global nature of the dataset, and the high quality of the documentation and code. Datasets often focus on a particular part of the world (USA mostly) and are often released without clear documentation on how they were produced, making them hard to extend or use with confidence. Therefore WorldStrat demonstrates best practice for publishing a dataset which I hope other publishers will emulate. Sentinel 2 imagery is widely used and so enhancement of its qualities through applications such as super resolution could have significant positive impact on end users. Finally I would like to thank Julien Cornebise for his feedback on the draft of this post.</p>

<h2 id="footnotes">
<a class="anchor" href="#footnotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://arxiv.org/abs/2207.06418Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>https://github.com/worldstrat/worldstratÂ <a href="#fnref:2" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>https://zenodo.org/record/6810792#.YtjNb-zMK3IÂ <a href="#fnref:3" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>https://www.kaggle.com/datasets/jucor1/worldstratÂ <a href="#fnref:4" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>https://eo-learn.readthedocs.io/en/latest/index.htmlÂ <a href="#fnref:5" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Alternatively I could have just created a notebook on Kaggle itself, and would not have needed to wait for a downloadÂ <a href="#fnref:6" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>https://arxiv.org/abs/1803.09010Â <a href="#fnref:7" class="reversefootnote" role="doc-backlink">â†©</a>Â <a href="#fnref:7:1" class="reversefootnote" role="doc-backlink">â†©<sup>2</sup></a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="robmarkcole/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/markdown/2022/08/01/worldstrat.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
