<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transformers in remote sensing | Blog of Robin Cole</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Transformers in remote sensing" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction to Transformers and why you should care" />
<meta property="og:description" content="Introduction to Transformers and why you should care" />
<link rel="canonical" href="https://robmarkcole.com/markdown/2022/08/15/transformers.html" />
<meta property="og:url" content="https://robmarkcole.com/markdown/2022/08/15/transformers.html" />
<meta property="og:site_name" content="Blog of Robin Cole" />
<meta property="og:image" content="https://robmarkcole.com/images/transformers/transformer.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-15T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://robmarkcole.com/images/transformers/transformer.jpg" />
<meta property="twitter:title" content="Transformers in remote sensing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-15T00:00:00-05:00","datePublished":"2022-08-15T00:00:00-05:00","description":"Introduction to Transformers and why you should care","headline":"Transformers in remote sensing","image":"https://robmarkcole.com/images/transformers/transformer.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://robmarkcole.com/markdown/2022/08/15/transformers.html"},"url":"https://robmarkcole.com/markdown/2022/08/15/transformers.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://robmarkcole.com/feed.xml" title="Blog of Robin Cole" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-235705291-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-235705291-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blog of Robin Cole</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Transformers in remote sensing</h1><p class="page-description">Introduction to Transformers and why you should care</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-08-15T00:00:00-05:00" itemprop="datePublished">
        Aug 15, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#transformers-in-remote-sensing">Transformers in remote sensing</a>
<ul>
<li class="toc-entry toc-h3"><a href="#performance">Performance</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#further-reading">Further reading</a></li>
<li class="toc-entry toc-h2"><a href="#terminology">Terminology</a></li>
<li class="toc-entry toc-h2"><a href="#footnotes">Footnotes</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>I will admit that whenever I hear Transformers, the first image that flashes through my mind is that of the cartoon characters I remember from my childhood in the 1980’s. And whilst that was, and continues to be a great TV &amp; film series, the Transformers I wish to talk about are those making waves in the deep learning community. So first things first, what is a Transformer? The Transformer is a neural network architecture first published in a 2017 paper titled <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> by researchers at Google. Transformers learn context by tracking relationships in sequential data using a technique called <em>attention</em>. Transformers are replacing CNN’s and RNN’s in many applications and have come to dominate the field of natural language processing<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> in only a few years.</p>

<p>Transformers were successfully applied to imagery in the 2020 paper <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>, again by researchers at Google. Transformers applied to vision are commonly referred to as Vision Transformers, or ViT. I will not go deep into deep technical details here since those are covered in the paper and elsewhere, but the first figure from the paper is worth reproducing here along with a basic explanation of the core features of ViT’s:</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/transformers/paper_fig1.jpg" alt="" title="Figure 1 from An Image is Worth 16x16 Words"></p>

<p>The caption from the figure above explains: <em>We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence.</em> We can see that there are absolutely <strong>no convolutional layers</strong>, and the authors note that the ViT <em>has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.</em> This results in ViT’s requiring larger data volumes to achieve comparable levels of performance to CNN’s. Indeed a ViT trained on ImageNet (1M images), under-performs a CNN. However with significantly larger data volumes (300M images), the ViT outperforms the CNN. Also a ViT is able to capture more global relationships in an image, which hints at their potential benefits when applied to remote sensing imagery. Note that in practice it is typical to fine tune a ViT that has been pre-trained, just as we usually do with a CNN.</p>

<h2 id="transformers-in-remote-sensing">
<a class="anchor" href="#transformers-in-remote-sensing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformers in remote sensing</h2>
<p>Relatively recently I have started noticing greater numbers of remote sensing publications using Transformers. I became curious about the level of interest and understanding of Transformers in the remote sensing community, and a Tweet about this got many responses:</p>

<blockquote class="twitter-tweet tw-align-center">
<p lang="en" dir="ltr">What questions do people have about Transformers and their use in remote sensing? This is the topic of my next blog post 🙇‍♂️🚀</p>— Robin Cole (@robmarkcole) <a href="https://twitter.com/robmarkcole/status/1554348041926311937?ref_src=twsrc%5Etfw">August 2, 2022</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>To paraphrase the questions asked:</p>
<ul>
  <li>Do Transformers have the best performance on common remote sensing tasks?</li>
  <li>Do they require more training data?</li>
  <li>Is there a minimum data volume requirement?</li>
  <li>Are they harder to train?</li>
  <li>Are there significant engineering tradeoffs in using Transformers?</li>
  <li>Are transformers of any use for tabular remote sensing data?</li>
</ul>

<h3 id="performance">
<a class="anchor" href="#performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance</h3>
<p>To begin addressing these questions I will first reference the 2022 paper <a href="add%20url">Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification</a>. This paper compares compares the performance of vision transformers (ViT’s) with eight other neural network architectures on the task of classification. Figure 1 from that paper is shown below:</p>

<p><img src="https://raw.githubusercontent.com/robmarkcole/blog/master/images/transformers/classification.jpg" alt="" title="Figure 1 from Current Trends in Deep Learning for Earth Observation"></p>

<p>This paper provides a useful overview of multiple classification datasets, shown in (a) above. The datasets range considerably in number of images and classes. Model performance is compared on (b) multi-label and (c) multi-class classification tasks. The darker shaded bars are performance when a model is trained from scratch, and the lighter shading bars are performance when the model is pre-trained on the ImageNet-1K dataset. We immediately observe that pre-training always improves model performance, which is a very useful takeaway. Amongst the models, ViT and DenseNet are the best performers, although ViT actually performs <em>worst</em> on the largest dataset, BigEarthNet. It is also interesting that for many of the datasets, the commonly used ResNet achieves comparable performance.</p>

<h2 id="further-reading">
<a class="anchor" href="#further-reading" aria-hidden="true"><span class="octicon octicon-link"></span></a>Further reading</h2>
<ul>
  <li>Original Google <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">blog post on transformers</a>
</li>
  <li>Pytorch implementation of a ViT: <a href="https://github.com/lucidrains/vit-pytorch">vit-pytorch</a>
</li>
  <li>ArXiv paper: <a href="https://arxiv.org/abs/2207.09238">Formal Algorithms for Transformers</a>
</li>
</ul>

<h2 id="terminology">
<a class="anchor" href="#terminology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Terminology</h2>
<ul>
  <li>CNN: convolutional neural network</li>
  <li>RNN: recurrent neural network</li>
  <li>ViT: vision transformer</li>
</ul>

<h2 id="footnotes">
<a class="anchor" href="#footnotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/ <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="robmarkcole/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/markdown/2022/08/15/transformers.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/robmarkcole" target="_blank" title="robmarkcole"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
