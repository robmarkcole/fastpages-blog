---
title: "A brief introduction to satellite image classification"
description: "Answers what is image classification, how to select a model and how to approach your first project"
layout: post
categories: [markdown]
toc: true
image: images/classification/classification-main.png
show_image: true
hide: true
comments: true
---
## Introduction
On [Twitter](https://twitter.com/robmarkcole) & [LinkedIn](https://www.linkedin.com/in/robmarkcole/) I regularly post on the topic of deep learning applied to satellite & aerial imagery, and I receive many messages from people who are new to the field and are seeking introductory level material. I maintain a popular repository on Github called the [satellite-image-deep-learning](https://github.com/robmarkcole/satellite-image-deep-learning) which lists many useful references, ranging from very introductory articles to the code for cutting edge techniques published in the academic literature. However this long list of references is not necessarily that approachable to people getting started, so I am going to publish a series of short blog posts introducing the main techniques listed in my repository. This first post is on image classification of satellite imagery, which has many practical applications and is ideal for practicing the basics of deep learning. There is no code to follow in this post, but an introduction to the topic and a general description of the process. I aim to publish usable notebooks in the near future. Lets get started!

## What is image classification?
You may already be familiar with image classification from seeing the numerous cats vs dogs image classification tutorials on the internet. Image classification is therefore the task of assigning a label to an image, or even assigning multiple labels to an image[^1]. When applied to satellite imagery, classification has two common uses:

- label the dominant subject of image, e.g. golf course, harbour
- perform binary detection of some subject, e.g. ship present or not, deforested or not

To get more familiar with this task I recommend exploring a couple of benchmark image classification datasets[^2], such as the UC Merced dataset (a sample of which is shown below) or the EuroSAT dataset. Both of these datasets are available in the standard RGB/single label format, but also in more interesting multi-class versions. For these and other datasets see the [Datasets](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/datasets.md) section on my repository.

![](https://www.researchgate.net/publication/324924412/figure/fig4/AS:644015246544898@1530556608631/Example-images-from-the-UC-Merced-dataset.png "The UC Merced dataset")

## Selecting & training image classification models
The [Classification](https://github.com/robmarkcole/satellite-image-deep-learning#classification) section in my repository lists many different resources demonstrating the training of classification models on satellite imagery. In fact it is relatively rare to train a model from scratch on your own dataset, and far more common to use a model that has been pre-trained on a benchmark dataset (usually [ImageNet](https://en.wikipedia.org/wiki/ImageNet)) and then fine-tune this model on your own dataset. To learn more about fine-tuning I recommend the [fine-tuning lesson on d2l.ai](https://d2l.ai/chapter_computer-vision/fine-tuning.html). In fine-tuning the feature extraction layers are frozen, and only the fully connected classification layers are updated:

![](https://api.intechopen.com/media/chapter/64395/media/F5.png "Strategy of fine-tuning deeper layers of AlexNet.")

So assuming your are going to fine-tune a model your fist choice is which model to select? The internet regularly reports new 'state of the art' models which improve performance on some benchmark dataset or other, and it would be reasonable to assume that you should choose the latest and greatest model. However for an approachable article comparing models I highly recommend reading [The best vision models for fine-tuning](https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning/notebook) by Jeremy Howard. In this article Jeremy compares 86 models on two benchmark datasets; the [IIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) and the [Kaggle Planet dataset](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data) (a remote sensing dataset). He shows that the modern models (the vit is a [Transformer model](https://robmarkcole.com/markdown/2022/08/15/transformers.html)) are indeed the top performers in terms of accuracy, shown in the table below:

![](https://raw.githubusercontent.com/robmarkcole/blog/master/images/classification/table.png "The top 10 performers on the Planet dataset")

Interestingly the best performers vary between the Pets and Planet datasets, and Jeremy attributes this to the fact that the Planet dataset does not resemble the images in the ImageNet dataset (which most models are pre-trained on), so the models which learn new features the fastest are the best performers on Planet. He also notes that "there's little correlation between model size and performance" on the Planet dataset, and advises selecting smaller models due to this (which will also be faster in use). An additional advantage of choosing a small model is that the pace of experimentation is faster. For me a surprising result on the Planet dataset is that the relatively old ([published in 2015](https://arxiv.org/abs/1512.03385)) Resnet 18 model is in the top 10 performers. As Jeremy says, "Resnet 18 has very low memory use, is fast, and is still quite accurate", and for these reasons I suggest it is a good default model to begin projects with. 

## How to approach your first classification project
Perhaps you already have a use case for classification from your day job, but if not I suggest deciding on a topic that interests you (e.g. deforestation, crop classification) and finding a relevant dataset on Kaggle or in my repository. There are also regular competitions run by organisations including ESA and the [Radiant Earth Foundation](https://www.radiant.earth/), and these typically provide a dataset and an exciting challenge. Begin by following a tutorial on fine-tuning a vision model (e.g. the [fine-tuning lesson on d2l.ai](https://d2l.ai/chapter_computer-vision/fine-tuning.html)) and then adapt it to use your chosen dataset. You will probably encounter some challenges just from switching dataset alone, such as dealing with different sized images or number of channels. If you are not particularly familiar with geospatial images (geotiffs) then I recommend sticking to datasets where the images are simply png or jpgs. If you do want to work with geospatial images you will probably need to 'chip' large images into smaller training chips, and I list many tools to do this on my repository [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/software.md#image-chippingtiling--merging). Once you have assembled your dataset you will probably have to modify the model training code for loading and preprocessing the dataset, and this is a good opportunity to practice your Python programming skills and get familiar with your chosen deep learning framework (Tensorflow or Pytorch). Note that the UC Merced & EuroSAT dataset can be accessed via the [Tensorflow](https://www.tensorflow.org/datasets/catalog/overview) data hub, simplifying the process of using this dataset. Pytorch users will want to access these datasets via [torchgeo](https://torchgeo.readthedocs.io/en/latest/_modules/torchgeo/datamodules/ucmerced.html), and will also benefit from much additional functionality that simplifies working with geospatial datasets.

Moving on to model fine-tuning, begin experimenting to see which factors improve or degrade model performance. You will find that data augmentation and training parameters such as batch size and number of epochs will have a significant impact on the models performance. I recommend reading about under and over-fitting, and performing experiments to demonstrate these issues so that you will recognise them when they occur. In my experience 
over-fitting is quite common, and you should visualise your training losses to identify it. The figure below is from the post [Underfitting and Overfitting in Deep Learning](https://medium.com/mlearning-ai/underfitting-and-overfitting-in-deep-learning-687b1b7eb738)

![](https://miro.medium.com/max/1400/1*pgQzuW_Wava2aHcVBsaXbw.png "Over fitting can be identified when training loss is decreasing but validation loss is increasing")

Once you have gained confidence tfine-tuning a model on a published dataset, you could move on to creating your own dataset. A classification dataset can be created by downloading images from Google Earth, using one of the scripts listed on my repository [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/software.md#image-dataset-creation). If you are unsure which tool to use I suggest first checking out [Map Tiles Downloader](https://github.com/AliFlux/MapTilesDownloader) which provides a helpful UI [^3]. To prepare the dataset for training it will be necessary to sort your images into folders where the folder name is the label that will be used for that class. Fortunately this can be done using just the file browser on your computer, and no special 'annotation' software is required. However if you are interested in using a cloud hosted service for sorting and splitting your data I recommend [Roboflow](https://roboflow.com/robincole). At this point you may wish to write a blog post about your project, or simply publish a notebook on Kaggle. I personally think that summarising and presenting your work is an important part of the learning process, and recommend doing it even for small projects. If you want to take your project to the next level, consider creating a web app or API to provide a live service which you can use to demonstrate the model. There are a few examples of how to do this in my repository [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/software.md#web-apps) and [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/deployment.md)

## Summary
I hope that this has been a useful introduction to satellite imagery classification, and provided a practical overview of how models are trained in practice. As mentioned in the introduction, I plan on publishing notebooks which you will actually be able to work through, and in the meantime I am just seeking feedback on what challenges people have getting started, so I can incorporate this into the notebooks. In the meantime I recommend joining up to the [satellite-image-deep-learning LinkedIn group](https://www.linkedin.com/groups/12698393/) where there is daily material on the topic of deep learning on satellite imagery, and I hope to hear back about peoples projects on this topic. 

## Footnotes
[^1]: When there are **more** than one label to be associated with an image the task is usually referred to as 'multi-class' or 'multi-label' classification. This more complex task is beyond the scope of this post but I aim to revisit in a later post. If you are keen to explore this topic already checkout the Medium article [Multi-label Land Cover Classification with Deep Learning](https://towardsdatascience.com/multi-label-land-cover-classification-with-deep-learning-d39ce2944a3d)
[^2]: A benchmark dataset is a dataset that is used as a standard by the community to compare the performance of different techniques
[^3]: My understanding is that it is perfectly legal to access Google Earth imagery in this way for non commercial purposes. If in doubt seek advice from your companies or universities legal team
