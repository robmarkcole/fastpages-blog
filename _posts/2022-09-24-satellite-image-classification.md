---
title: "A brief introduction to satellite image classification with neural networks"
description: "What is image classification, which models are used and how to approach your first project"
layout: post
categories: [markdown]
toc: true
image: images/classification/classification-main.png
show_image: true
hide: true
comments: true
---
## Introduction
On [Twitter](https://twitter.com/robmarkcole) & [LinkedIn](https://www.linkedin.com/in/robmarkcole/) I regularly post on the topic of deep learning applied to satellite & aerial imagery, and I receive many messages from people who are new to the field and are seeking introductory level material. I maintain a popular repository on Github called the [satellite-image-deep-learning](https://github.com/robmarkcole/satellite-image-deep-learning) which lists many useful references, ranging from very introductory articles to the code for cutting edge techniques published in the academic literature. However this long list of references is not necessarily that approachable to people getting started, so I am going to publish a series of short blog posts introducing the main techniques listed in my repository. This first post is on image classification of satellite imagery, which has many practical applications and is ideal for practicing the basics of deep learning. There is no code to follow in this post, but an introduction to the topic and a general description of the process. I aim to publish usable notebooks in the near future. Lets get started!

## What is image classification?
You may already be familiar with image classification from seeing the numerous cats vs dogs image classification tutorials on the internet. Image classification is therefore the task of assigning a single label to an image, or even assigning multiple labels to an image[^1]. Note however that the term 'classification' can mean different things to different people - in particular in many articles classification may be used to describe pixel or pixel-cluster level labels, which I would call semantic segmentation. To be clear here we are discussing single labels applied to single images, using deep learning neural networks to generate the label. 

When applied to satellite imagery, single label classification has two common uses:

- label the dominant subject of image, e.g. golf course, harbour
- perform binary detection of some subject, e.g. ship present or not, deforested or not

There are also more advanced classification techniques, for example using a time-series of images to classify crops where the unique seasonal changes are a strong indicator of crop type. I aim to create a later blog post discussing these techniques also, but if you are interested to read more these are under the [Time series](https://github.com/robmarkcole/satellite-image-deep-learning#time-series) section of my repository.

## Image classification datasets
To get more familiar with satellite image classification I recommend exploring a couple of benchmark datasets[^2], such as the UC Merced dataset (a sample of which is shown below) or the EuroSAT dataset. Both of these datasets are available in the standard RGB/single label format, but also in more interesting multi-class versions. For these and other datasets see the [Datasets](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/datasets.md) section on my repository.

![](https://www.researchgate.net/publication/324924412/figure/fig4/AS:644015246544898@1530556608631/Example-images-from-the-UC-Merced-dataset.png "The UC Merced dataset")

## Selecting & training models
The [Classification](https://github.com/robmarkcole/satellite-image-deep-learning#classification) section in my repository lists many different resources demonstrating the training of classification models on satellite imagery. In fact it is relatively rare to train a model from scratch on your own dataset, and far more common to use a model that has been pre-trained on a benchmark dataset (usually [ImageNet](https://en.wikipedia.org/wiki/ImageNet)) and then fine-tune this model on your own dataset. To learn more about fine-tuning I recommend the [fine-tuning lesson on d2l.ai](https://d2l.ai/chapter_computer-vision/fine-tuning.html). In fine-tuning the feature extraction layers are frozen, and only the fully connected classification layers are updated:

![](https://api.intechopen.com/media/chapter/64395/media/F5.png "Strategy of fine-tuning deeper layers of AlexNet.")

The internet regularly reports new 'state of the art' models which improve performance on some benchmark dataset or other, and it would be reasonable to assume that the latest and greatest models are usually used in applications. However for an approachable article comparing models I highly recommend reading [The best vision models for fine-tuning](https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning/notebook) by Jeremy Howard. In this article Jeremy compares 86 models on two benchmark datasets; the [IIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) and the [Kaggle Planet dataset](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data) (a remote sensing dataset). He shows that the modern models (the vit is a [Transformer model](https://robmarkcole.com/markdown/2022/08/15/transformers.html)) are indeed the top performers in terms of accuracy, shown in the table below:

![](https://raw.githubusercontent.com/robmarkcole/blog/master/images/classification/table.png "The top 10 performers on the Planet dataset")

Interestingly the best performers vary between the Pets and Planet datasets, and Jeremy attributes this to the fact that the Planet dataset does not resemble the images in the ImageNet dataset (which most models are pre-trained on), so the models which learn new features the fastest are the best performers. He also notes that "there's little correlation between model size and performance" on the Planet dataset, and therefore advises selecting smaller models (which will also be faster in use). An additional advantage of choosing a small model is that the pace of experimentation is faster. For me a surprising result on the Planet dataset is that the relatively old ([published in 2015](https://arxiv.org/abs/1512.03385)) Resnet 18 model is in the top 10 performers. As Jeremy says, "Resnet 18 has very low memory use, is fast, and is still quite accurate", and for these reasons I suggest it is a good default model to begin projects with. 

## How to approach your first classification project
Perhaps you already have a use case for classification from your day job, but if not I suggest deciding on a topic that interests you (e.g. deforestation, crop classification) and finding a relevant dataset on [Kaggle](https://www.kaggle.com/), [Roboflow data hub](https://roboflow.com/robincoledata), or in my repository. There are also regular competitions run by organisations including ESA and the [Radiant Earth Foundation](https://www.radiant.earth/), and these typically provide a dataset and an exciting challenge. Begin by following a tutorial on fine-tuning a vision model (e.g. the [fine-tuning lesson on d2l.ai](https://d2l.ai/chapter_computer-vision/fine-tuning.html)) and then adapt it to use your chosen dataset. You will probably encounter some challenges just from switching dataset alone, such as dealing with different sized images or number of channels. If you are not particularly familiar with geospatial images (geotiffs) then I recommend sticking to datasets where the images are simply png or jpgs. If you do want to work with geospatial images you will probably need to 'chip' large images into smaller training chips, and I list many tools to do this on my repository [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/software.md#image-chippingtiling--merging). Once you have assembled your dataset you will probably have to modify the model training code for loading and preprocessing the dataset, and this is a good opportunity to practice your Python programming skills and get familiar with your chosen deep learning framework (Tensorflow or Pytorch). Note that the UC Merced & EuroSAT dataset can be accessed via the [Tensorflow](https://www.tensorflow.org/datasets/catalog/overview) data hub, simplifying the process of using this dataset. Pytorch users will want to access these datasets via [torchgeo](https://torchgeo.readthedocs.io/en/latest/_modules/torchgeo/datamodules/ucmerced.html), and will also benefit from much additional functionality that simplifies working with geospatial datasets.

Moving on to model fine-tuning, begin experimenting to see which factors improve or degrade model performance. You will find that data augmentation and training parameters such as batch size and number of epochs will have a significant impact on the models performance. As a general guide, the classification accuracy you can achieve roughly depends on three factors:

-  The quality of the input images; including appropriate image pre-processing, spatial & radiometric resolution of the images
-  Quality, quantity and balance of the training dataset and labels
-  Selection and fine-tuning of the deep learning model

Keep iterating with these parameters until you feel confident in fine-tuning a model on a published dataset. Next you could move on to creating your own dataset, and a classification dataset can be created by downloading images from Google Earth using one of the scripts listed on my repository [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/software.md#image-dataset-creation). If you are unsure which tool to use I suggest first checking out [Map Tiles Downloader](https://github.com/AliFlux/MapTilesDownloader) which provides a helpful UI [^3]. To prepare the dataset for training it will be necessary to sort your images into folders where the folder name is the label that will be used for that class. Fortunately this can be done using just the file browser on your computer, and no special 'annotation' software is required. If you're interested in a tool to better understand and curate your classification data, I recommend [Roboflow](https://roboflow.com/robincole). You can also use Roboflow to have a hosted model API after training your custom model.

At this point you may wish to write a blog post about your project, or simply publish a notebook on Kaggle. I personally think that summarising and presenting your work is an important part of the learning process, and recommend doing it even for small projects. If you want to take your project to the next level, consider creating a web app or API to provide a live service which you can use to demonstrate the model. There are a few examples of how to do this in my repository [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/software.md#web-apps) and [here](https://github.com/robmarkcole/satellite-image-deep-learning/blob/master/assets/deployment.md). Note that if you want to deploy a production ready service there may be a significant amount of engineering required to handle pre-processing of the uploaded images, for example to handle multiple images types, detect quality issues etc. This is beyond the scope of this post but is another topic I am considering doing a post on.

## Summary
I hope that this has been a useful introduction to satellite imagery classification, and provided an interesting overview of how models are trained in practice. As mentioned in the introduction, in the future I plan on publishing notebooks which you will actually be able to work through. In the meantime I am seeking feedback on what challenges people have getting started, so I can incorporate this into the notebooks. Feel free to drop your feedback in the comments section below, or send me a message online. In the meantime I recommend joining up to the [satellite-image-deep-learning LinkedIn group](https://www.linkedin.com/groups/12698393/) where there is daily material on the topic of deep learning on satellite imagery. Thanks for reading this far and I look forward to connecting soon! 

## Footnotes
[^1]: When there are **more** than one label to be associated with an image the task is usually referred to as 'multi-class' or 'multi-label' classification. This more complex task is beyond the scope of this post but I aim to revisit in a later post. If you are keen to explore this topic already checkout the Medium article [Multi-label Land Cover Classification with Deep Learning](https://towardsdatascience.com/multi-label-land-cover-classification-with-deep-learning-d39ce2944a3d)
[^2]: A benchmark dataset is a dataset that is used as a standard by the community to compare the performance of different techniques
[^3]: My understanding is that it is perfectly legal to access Google Earth imagery in this way for non commercial purposes. If in doubt seek advice from your company or university legal team
